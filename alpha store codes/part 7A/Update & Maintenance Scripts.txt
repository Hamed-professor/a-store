#!/bin/bash
# Alpha Store Update & Maintenance Script (scripts/update.sh)

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
BACKUP_DIR="/backups/alpha-store"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_FILE="/var/log/alpha-store/update.log"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log() {
    local message="[$(date +'%Y-%m-%d %H:%M:%S')] $1"
    echo -e "${BLUE}$message${NC}"
    echo "$message" >> "$LOG_FILE"
}

error() {
    local message="[ERROR] $1"
    echo -e "${RED}$message${NC}" >&2
    echo "$message" >> "$LOG_FILE"
}

success() {
    local message="[SUCCESS] $1"
    echo -e "${GREEN}$message${NC}"
    echo "$message" >> "$LOG_FILE"
}

warning() {
    local message="[WARNING] $1"
    echo -e "${YELLOW}$message${NC}"
    echo "$message" >> "$LOG_FILE"
}

# Check if running as root
check_root() {
    if [ "$EUID" -ne 0 ]; then
        error "Please run this script as root"
        exit 1
    fi
}

# Create pre-update backup
create_backup() {
    log "Creating pre-update backup..."
    bash "$SCRIPT_DIR/backup.sh" backup
    if [ $? -eq 0 ]; then
        success "Pre-update backup completed"
    else
        error "Pre-update backup failed"
        exit 1
    fi
}

# Update system packages
update_system() {
    log "Updating system packages..."
    
    apt-get update -y
    apt-get upgrade -y
    apt-get autoremove -y
    apt-get autoclean
    
    success "System packages updated"
}

# Update Docker and Docker Compose
update_docker() {
    log "Checking Docker updates..."
    
    # Update Docker
    local docker_version=$(docker --version | cut -d' ' -f3 | cut -d',' -f1)
    log "Current Docker version: $docker_version"
    
    # Update Docker Compose
    local compose_version=$(docker-compose --version | cut -d' ' -f3 | cut -d',' -f1)
    log "Current Docker Compose version: $compose_version"
    
    # Pull latest Docker images
    log "Pulling latest Docker images..."
    cd "$PROJECT_ROOT"
    docker-compose -f docker-compose.prod.yml pull
    
    success "Docker components updated"
}

# Update SSL certificates
update_ssl() {
    log "Checking SSL certificate renewal..."
    
    certbot renew --quiet --post-hook "docker exec alpha-store-nginx nginx -s reload"
    
    if [ $? -eq 0 ]; then
        success "SSL certificates checked/renewed"
    else
        warning "SSL certificate renewal check completed with warnings"
    fi
}

# Update application code
update_application() {
    log "Updating Alpha Store application..."
    
    cd "$PROJECT_ROOT"
    
    # Rebuild images with latest code
    log "Building updated Docker images..."
    docker-compose -f docker-compose.prod.yml build --no-cache
    
    # Rolling update with zero downtime
    log "Performing rolling update..."
    
    # Update backend first
    docker-compose -f docker-compose.prod.yml up -d --no-deps backend
    sleep 30
    
    # Update frontend
    docker-compose -f docker-compose.prod.yml up -d --no-deps frontend
    sleep 30
    
    # Update admin
    docker-compose -f docker-compose.prod.yml up -d --no-deps admin
    sleep 30
    
    # Update nginx (if needed)
    docker-compose -f docker-compose.prod.yml up -d --no-deps nginx
    sleep 15
    
    success "Application updated successfully"
}

# Database maintenance
maintain_database() {
    log "Running database maintenance..."
    
    # MongoDB maintenance
    docker exec alpha-store-mongodb mongo alphastore_prod --eval "
        db.runCommand({compact: 'products'});
        db.runCommand({compact: 'orders'});
        db.runCommand({compact: 'users'});
        db.runCommand({reIndex: 'products'});
        db.runCommand({reIndex: 'orders'});
        db.runCommand({reIndex: 'users'});
    " 2>/dev/null || warning "MongoDB maintenance completed with warnings"
    
    # Redis maintenance
    docker exec alpha-store-redis redis-cli BGREWRITEAOF 2>/dev/null || warning "Redis AOF rewrite failed"
    
    success "Database maintenance completed"
}

# Clear caches
clear_caches() {
    log "Clearing application caches..."
    
    # Clear Redis cache
    docker exec alpha-store-redis redis-cli FLUSHDB
    
    # Clear Nginx cache
    docker exec alpha-store-nginx find /var/cache/nginx -type f -delete 2>/dev/null || true
    
    # Clear application cache
    docker exec alpha-store-backend node -e "
        const redis = require('redis');
        const client = redis.createClient(process.env.REDIS_URL);
        client.flushall();
        client.quit();
    " 2>/dev/null || warning "Application cache clearing completed with warnings"
    
    success "Caches cleared"
}

# Update security configurations
update_security() {
    log "Updating security configurations..."
    
    # Update firewall rules
    bash "$PROJECT_ROOT/security/firewall-rules.sh"
    
    # Restart fail2ban
    systemctl restart fail2ban 2>/dev/null || warning "Fail2ban restart failed"
    
    # Update security headers
    docker exec alpha-store-nginx nginx -t && docker exec alpha-store-nginx nginx -s reload
    
    success "Security configurations updated"
}

# Performance optimization
optimize_performance() {
    log "Running performance optimizations..."
    
    # Docker system cleanup
    docker system prune -f
    docker volume prune -f
    docker image prune -f
    
    # Log rotation
    logrotate -f /etc/logrotate.d/alpha-store
    
    # Optimize images (if needed)
    find "$PROJECT_ROOT/uploads" -name "*.jpg" -o -name "*.jpeg" -o -name "*.png" | head -100 | while read image; do
        if command -v jpegoptim &> /dev/null && [[ "$image" =~ \.(jpg|jpeg)$ ]]; then
            jpegoptim --max=85 "$image" 2>/dev/null || true
        fi
        if command -v optipng &> /dev/null && [[ "$image" =~ \.png$ ]]; then
            optipng -o2 "$image" 2>/dev/null || true
        fi
    done
    
    success "Performance optimization completed"
}

# Run health checks
run_health_checks() {
    log "Running post-update health checks..."
    
    bash "$SCRIPT_DIR/health-check.sh"
    
    if [ $? -eq 0 ]; then
        success "All health checks passed"
    else
        error "Some health checks failed"
        return 1
    fi
}

# Send update notification
send_notification() {
    local status=$1
    local message="Alpha Store update completed successfully at $(date)"
    
    if [ "$status" != "success" ]; then
        message="Alpha Store update failed at $(date). Check logs for details."
    fi
    
    # Send email notification (if configured)
    if command -v mail &> /dev/null && [ -n "${ADMIN_EMAIL:-}" ]; then
        echo "$message" | mail -s "Alpha Store Update Notification" "$ADMIN_EMAIL"
    fi
    
    # Log to system log
    logger -t alpha-store-update "$message"
    
    log "Notification sent: $message"
}

# Rollback function
rollback() {
    local backup_timestamp=$1
    
    error "Rolling back to backup: $backup_timestamp"
    
    # Stop current services
    cd "$PROJECT_ROOT"
    docker-compose -f docker-compose.prod.yml down
    
    # Restore from backup
    if [ -f "$BACKUP_DIR/database/mongodb_$backup_timestamp.tar.gz" ]; then
        log "Restoring database..."
        tar -xzf "$BACKUP_DIR/database/mongodb_$backup_timestamp.tar.gz" -C /tmp/
        docker-compose -f docker-compose.prod.yml up -d mongodb
        sleep 30
        # Restore MongoDB data here
        docker exec alpha-store-mongodb mongorestore /tmp/mongodb_$backup_timestamp/
        rm -rf /tmp/mongodb_$backup_timestamp
    fi
    
    # Restore configuration
    if [ -f "$BACKUP_DIR/config/config_$backup_timestamp.tar.gz" ]; then
        log "Restoring configuration..."
        tar -xzf "$BACKUP_DIR/config/config_$backup_timestamp.tar.gz" -C "$PROJECT_ROOT"
    fi
    
    # Start services
    docker-compose -f docker-compose.prod.yml up -d
    
    error "Rollback completed"
}

# Main update function
main() {
    log "Starting Alpha Store update process..."
    
    local failed=0
    local start_time=$(date +%s)
    
    # Create log directory
    mkdir -p "$(dirname "$LOG_FILE")"
    
    # Maintenance mode (optional)
    if [ "${MAINTENANCE_MODE:-true}" = "true" ]; then
        log "Enabling maintenance mode..."
        touch "$PROJECT_ROOT/maintenance.flag"
    fi
    
    # Run update steps
    create_backup || ((failed++))
    update_system || ((failed++))
    update_docker || ((failed++))
    update_ssl || ((failed++))
    update_application || ((failed++))
    maintain_database || ((failed++))
    clear_caches || ((failed++))
    update_security || ((failed++))
    optimize_performance || ((failed++))
    
    # Disable maintenance mode
    if [ -f "$PROJECT_ROOT/maintenance.flag" ]; then
        log "Disabling maintenance mode..."
        rm "$PROJECT_ROOT/maintenance.flag"
    fi
    
    # Run health checks
    sleep 60 # Wait for services to stabilize
    run_health_checks || ((failed++))
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $failed -eq 0 ]; then
        success "Update process completed successfully in ${duration}s!"
        send_notification "success"
        exit 0
    else
        error "Update process failed with $failed error(s) in ${duration}s"
        send_notification "failed"
        
        # Ask for rollback
        read -p "Do you want to rollback? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            # Find latest backup
            local latest_backup=$(ls -1 "$BACKUP_DIR"/config/config_*.tar.gz 2>/dev/null | tail -1 | sed 's/.*config_\([0-9_]*\)\.tar\.gz/\1/')
            if [ -n "$latest_backup" ]; then
                rollback "$latest_backup"
            else
                error "No backup found for rollback"
            fi
        fi
        exit 1
    fi
}

# Parse command line arguments
case "${1:-update}" in
    "update")
        check_root
        main
        ;;
    "system")
        check_root
        update_system
        ;;
    "docker")
        check_root
        update_docker
        ;;
    "ssl")
        check_root
        update_ssl
        ;;
    "app")
        check_root
        update_application
        ;;
    "maintenance")
        check_root
        maintain_database
        clear_caches
        optimize_performance
        ;;
    "security")
        check_root
        update_security
        ;;
    "rollback")
        check_root
        if [ -z "${2:-}" ]; then
            error "Please specify backup timestamp: $0 rollback YYYYMMDD_HHMMSS"
            exit 1
        fi
        rollback "$2"
        ;;
    "health")
        run_health_checks
        ;;
    *)
        echo "Usage: $0 {update|system|docker|ssl|app|maintenance|security|rollback|health}"
        echo ""
        echo "Commands:"
        echo "  update      - Full system update (default)"
        echo "  system      - Update system packages only"
        echo "  docker      - Update Docker components only"
        echo "  ssl         - Update SSL certificates only"
        echo "  app         - Update application only"
        echo "  maintenance - Run database maintenance and optimization"
        echo "  security    - Update security configurations"
        echo "  rollback    - Rollback to specified backup"
        echo "  health      - Run health checks"
        exit 1
        ;;
esac

# =================================================================
# Grafana Dashboard Configuration (monitoring/grafana-dashboard.json)
# =================================================================

{
  "dashboard": {
    "id": null,
    "title": "Alpha Store Production Dashboard",
    "tags": ["alpha-store", "production"],
    "timezone": "Asia/Tehran",
    "panels": [
      {
        "id": 1,
        "title": "System Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "up",
            "legendFormat": "Services Online"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "palette-classic"
            },
            "custom": {
              "displayMode": "list",
              "orientation": "horizontal"
            },
            "mappings": [],
            "thresholds": {
              "steps": [
                {
                  "color": "green",
                  "value": null
                },
                {
                  "color": "red",
                  "value": 80
                }
              ]
            }
          }
        },
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        }
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ],
        "yAxes": [
          {
            "unit": "s"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        }
      },
      {
        "id": 3,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ],
        "yAxes": [
          {
            "unit": "reqps"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        }
      },
      {
        "id": 4,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "Error Rate"
          }
        ],
        "yAxes": [
          {
            "unit": "percentunit"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        }
      },
      {
        "id": 5,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "process_resident_memory_bytes",
            "legendFormat": "{{job}}"
          }
        ],
        "yAxes": [
          {
            "unit": "bytes"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 16
        }
      },
      {
        "id": 6,
        "title": "CPU Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(process_cpu_seconds_total[5m]) * 100",
            "legendFormat": "{{job}}"
          }
        ],
        "yAxes": [
          {
            "unit": "percent"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 16
        }
      },
      {
        "id": 7,
        "title": "Database Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "mongodb_connections",
            "legendFormat": "MongoDB Connections"
          },
          {
            "expr": "redis_connected_clients",
            "legendFormat": "Redis Connections"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 24
        }
      },
      {
        "id": 8,
        "title": "Disk Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "(1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100",
            "legendFormat": "Disk Usage %"
          }
        ],
        "yAxes": [
          {
            "unit": "percent"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 24
        }
      }
    ],
    "refresh": "5s",
    "schemaVersion": 27,
    "version": 0
  }
}

# =================================================================
# Redis Configuration (config/redis.conf)
# =================================================================

# Redis Production Configuration for Alpha Store

# Network
bind 0.0.0.0
port 6379
tcp-backlog 511
timeout 0
tcp-keepalive 300

# Security
requirepass ${REDIS_PASSWORD}
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command EVAL ""
rename-command DEBUG ""
rename-command CONFIG "CONFIG_a1b2c3d4e5"

# Memory Management
maxmemory 256mb
maxmemory-policy allkeys-lru
maxmemory-samples 5

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes
dbfilename dump.rdb
dir /data

# AOF
appendonly yes
appendfilename "appendonly.aof"
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
aof-load-truncated yes
aof-use-rdb-preamble yes

# Logging
loglevel notice
logfile /data/redis.log
syslog-enabled yes
syslog-ident redis

# Client Management
timeout 300
tcp-keepalive 300
maxclients 1000

# Performance
hz 10
dynamic-hz yes

# Slow Log
slowlog-log-slower-than 10000
slowlog-max-len 128

# Latency Monitor
latency-monitor-threshold 100

# =================================================================
# MongoDB Initialization Script (scripts/mongo-init.js)
# =================================================================

// MongoDB Initialization Script for Alpha Store Production

// Switch to admin database
db = db.getSiblingDB('admin');

// Create application user
db.createUser({
  user: process.env.MONGODB_USERNAME || 'alphastore',
  pwd: process.env.MONGODB_PASSWORD || 'secure_password_here',
  roles: [
    {
      role: 'readWrite',
      db: 'alphastore_prod'
    },
    {
      role: 'dbAdmin',
      db: 'alphastore_prod'
    }
  ]
});

// Switch to application database
db = db.getSiblingDB('alphastore_prod');

// Create collections with validation
db.createCollection('users', {
  validator: {
    $jsonSchema: {
      bsonType: 'object',
      required: ['email', 'password', 'name', 'role'],
      properties: {
        email: {
          bsonType: 'string',
          pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}
        },
        phone: {
          bsonType: 'string',
          pattern: '^09[0-9]{9}
        },
        role: {
          enum: ['customer', 'admin', 'manager']
        },
        status: {
          enum: ['active', 'inactive', 'suspended']
        }
      }
    }
  }
});

db.createCollection('products', {
  validator: {
    $jsonSchema: {
      bsonType: 'object',
      required: ['name', 'price', 'category', 'status'],
      properties: {
        name: {
          bsonType: 'string',
          minLength: 2,
          maxLength: 200
        },
        price: {
          bsonType: 'number',
          minimum: 0
        },
        status: {
          enum: ['active', 'inactive', 'draft']
        }
      }
    }
  }
});

db.createCollection('orders', {
  validator: {
    $jsonSchema: {
      bsonType: 'object',
      required: ['userId', 'items', 'total', 'status'],
      properties: {
        total: {
          bsonType: 'number',
          minimum: 0
        },
        status: {
          enum: ['pending', 'confirmed', 'processing', 'shipped', 'delivered', 'cancelled']
        }
      }
    }
  }
});

db.createCollection('categories');
db.createCollection('payments');
db.createCollection('reviews');
db.createCollection('coupons');
db.createCollection('inventory');
db.createCollection('analytics');

// Create indexes for performance
print('Creating indexes...');

// Users indexes
db.users.createIndex({ 'email': 1 }, { unique: true });
db.users.createIndex({ 'phone': 1 }, { sparse: true, unique: true });
db.users.createIndex({ 'role': 1 });
db.users.createIndex({ 'status': 1 });
db.users.createIndex({ 'createdAt': 1 });

// Products indexes
db.products.createIndex({ 'name': 1 });
db.products.createIndex({ 'category': 1 });
db.products.createIndex({ 'price': 1 });
db.products.createIndex({ 'status': 1 });
db.products.createIndex({ 'featured': 1 });
db.products.createIndex({ 'tags': 1 });
db.products.createIndex({ 'createdAt': -1 });
db.products.createIndex({ 'updatedAt': -1 });

// Text indexes for Persian search
db.products.createIndex({
  'name': 'text',
  'description': 'text',
  'tags': 'text'
}, {
  default_language: 'none',
  name: 'product_text_search'
});

// Orders indexes
db.orders.createIndex({ 'userId': 1 });
db.orders.createIndex({ 'status': 1 });
db.orders.createIndex({ 'createdAt': -1 });
db.orders.createIndex({ 'orderNumber': 1 }, { unique: true });
db.orders.createIndex({ 'paymentStatus': 1 });

// Categories indexes
db.categories.createIndex({ 'name': 1 }, { unique: true });
db.categories.createIndex({ 'slug': 1 }, { unique: true });
db.categories.createIndex({ 'parent': 1 });
db.categories.createIndex({ 'status': 1 });

// Payments indexes
db.payments.createIndex({ 'orderId': 1 });
db.payments.createIndex({ 'userId': 1 });
db.payments.createIndex({ 'gateway': 1 });
db.payments.createIndex({ 'status': 1 });
db.payments.createIndex({ 'createdAt': -1 });

// Reviews indexes
db.reviews.createIndex({ 'productId': 1 });
db.reviews.createIndex({ 'userId': 1 });
db.reviews.createIndex({ 'status': 1 });
db.reviews.createIndex({ 'rating': 1 });

// Analytics indexes
db.analytics.createIndex({ 'type': 1, 'date': -1 });
db.analytics.createIndex({ 'userId': 1, 'date': -1 });
db.analytics.createIndex({ 'productId': 1, 'date': -1 });

print('Database initialization completed successfully!');

# =================================================================
# Rate Limiting Configuration (security/rate-limits.conf)
# =================================================================

# Rate Limiting Rules for Alpha Store

# General API rate limiting
location /api/ {
    limit_req zone=api burst=30 nodelay;
    limit_req_status 429;
    
    # Custom error page for rate limiting
    error_page 429 @rate_limit_error;
}

# Authentication endpoints
location ~ ^/api/(login|register|reset-password|verify) {
    limit_req zone=auth burst=3 nodelay;
    limit_req_status 429;
    
    # Block after too many failed attempts
    location ~ ^/api/(login|reset-password) {
        limit_req zone=auth burst=3 nodelay;
        
        # Additional security for login
        if ($request_method = POST) {
            access_log /var/log/nginx/auth_attempts.log security;
        }
    }
}

# Admin panel rate limiting
location /admin/ {
    limit_req zone=admin burst=5 nodelay;
    limit_req_status 429;
    
    # Extra security for admin
    allow 192.168.1.0/24;    # Internal network
    allow 10.0.0.0/8;        # Private network
    deny all;                # Deny all other IPs (comment out for production)
}

# Payment endpoints (stricter limits)
location ~ ^/api/payment/ {
    limit_req zone=api burst=5 nodelay;
    limit_req_status 429;
    
    # Log all payment attempts
    access_log /var/log/nginx/payment_requests.log security;
}

# File upload endpoints
location ~ ^/api/upload {
    limit_req zone=api burst=10 nodelay;
    client_max_body_size 10M;
    client_body_timeout 60s;
}

# Search endpoints
location ~ ^/api/search {
    limit_req zone=api burst=20 nodelay;
    
    # Cache search results
    proxy_cache_valid 200 5m;
    proxy_cache_key "$scheme$request_method$host$request_uri";
}

# Rate limit error handler
location @rate_limit_error {
    internal;
    add_header Content-Type application/json always;
    add_header Retry-After 60 always;
    return 429 '{"error":"درخواست‌های شما بیش از حد مجاز است. لطفا کمی صبر کنید.","code":"RATE_LIMIT_EXCEEDED","retry_after":60}';
}

# Connection limits
limit_conn_zone $binary_remote_addr zone=perip:10m;
limit_conn_zone $server_name zone=perserver:10m;

# Apply connection limits
limit_conn perip 10;
limit_conn perserver 1000;